{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "local-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "import collections\n",
    "import re\n",
    "import jieba\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "american-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('bert-log')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\n",
    "fh = logging.FileHandler('bert.txt')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "final-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sapphire-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from importlib import reload\n",
    "import config\n",
    "reload(config)\n",
    "cnn_config = config.CNNConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fifth-touch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.618 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# def feed_data(x_batch, y_batch, keep_prob):\n",
    "#     feed_dict = {\n",
    "#         model.input_x: x_batch,\n",
    "#         model.input_y: y_batch,\n",
    "#         model.keep_prob: keep_prob\n",
    "#     }\n",
    "#     return feed_dict\n",
    "\n",
    "# def feed_data(x_batch, y_batch, keep_prob):\n",
    "#     feed_dict = {\n",
    "#         model.input_x: x_batch,\n",
    "#         model.input_y: y_batch,\n",
    "#         model.keep_prob: keep_prob\n",
    "#     }\n",
    "#     return feed_dict\n",
    "\n",
    "def read_file(file_dir, train=True):\n",
    "    \"\"\"\n",
    "    读取csv文件\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        comments, labels = [], []\n",
    "        df_train = pd.read_csv(file_dir, sep='\\t')\n",
    "        for comment, label in zip(df_train.comment, df_train.label):\n",
    "            comments.append(comment)\n",
    "            labels.append(label)\n",
    "        return comments, labels\n",
    "    else:\n",
    "        comments, comment_ids = [], []\n",
    "        df_test = pd.read_csv(file_dir)\n",
    "        for comment, comment_id in zip(df_test.comment, df_test.id):\n",
    "            comments.append(comment)\n",
    "            comment_ids.append(comment_id)\n",
    "        return comments, comment_ids\n",
    "    \n",
    "def build_vocab(train_comments, vocab_size=5000):\n",
    "    \"\"\"\n",
    "    对训练集分词,统计词频并取vocab_size个词作为词表\n",
    "    \"\"\"\n",
    "    \n",
    "    all_words = []\n",
    "    \n",
    "    for comment in train_comments:\n",
    "        seg_comment = jieba.lcut(comment)\n",
    "        all_words.extend(seg_comment)\n",
    "    word_counter = collections.Counter(all_words)\n",
    "    select_words = word_counter.most_common(vocab_size-2)\n",
    "    select_words, _ = list(zip(*select_words))\n",
    "    select_words = [\"<PAD>\"] + [\"<UNK>\"] + list(select_words)\n",
    "    word2id = {word: idx for idx, word in enumerate(select_words)}\n",
    "    \n",
    "    return word2id\n",
    "    \n",
    "def convert_to_inputids(comments, word2id, max_len=40):\n",
    "    \n",
    "    input_ids = []\n",
    "    \n",
    "    for comment in comments:\n",
    "        comment_ids = [word2id[w] if w in word2id  else word2id[\"<UNK>\"] for w in comment]\n",
    "        input_ids.append(comment_ids)\n",
    "        \n",
    "    input_ids = kr.preprocessing.sequence.pad_sequences(input_ids, max_len, padding='post')\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "def batch_iter(x, y, batch_size=32, shuffle=False):\n",
    "    \"\"\"\n",
    "    batch数据生成器\n",
    "    \"\"\"\n",
    "\n",
    "    sample_len = len(x)\n",
    "    batch_count = math.ceil(sample_len / batch_size)\n",
    "    # 随机打散\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(np.arange(sample_len))\n",
    "    else:\n",
    "        indices = list(np.arange(sample_len))\n",
    "    x_shuffle = np.array(x)[indices]\n",
    "    y_shuffle = np.array(y)[indices]\n",
    "\n",
    "    while True:\n",
    "        for i in range(batch_count):\n",
    "            start_id = i * batch_size\n",
    "            end_id = min((i + 1) * batch_size, sample_len)\n",
    "            yield x_shuffle[start_id: end_id], y_shuffle[start_id: end_id]  \n",
    "            \n",
    "def test_batch_iter(x, batch_size=32):\n",
    "    \n",
    "    sample_len = len(x)\n",
    "    batch_count = math.ceil(sample_len / batch_size)\n",
    "    \n",
    "    x = np.array(x)\n",
    "    while True:    \n",
    "        for i in range(batch_count):\n",
    "            start_id = i * batch_size\n",
    "            end_id = min((i + 1) * batch_size, sample_len)\n",
    "            yield x[start_id: end_id]\n",
    "\n",
    "train_comments, labels = read_file(cnn_config.train_dir)\n",
    "word2id = build_vocab(train_comments)\n",
    "train_ids = convert_to_inputids(train_comments, word2id, 50)\n",
    "data_iter = batch_iter(train_ids, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unique-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_comments, test_comment_ids = read_file(test_path, train=False)\n",
    "# test_ids = convert_to_inputids(test_comments, word2id, 50)\n",
    "# test_loader = test_batch_iter(test_ids, rnn_config.test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rough-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exotic-commissioner",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \n",
    "    def __init__(self, config, init_emd=None, is_training=True, is_testing=False):\n",
    "        \n",
    "        self.config = config\n",
    "        self.init_emd = init_emd\n",
    "        self.is_training = is_training\n",
    "        self.is_testing = is_testing\n",
    "        self.learning_rate = tf.Variable(self.config.learning_rate, trainable=False)\n",
    "        \n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.int32, name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.int32, name='keep_prob')\n",
    "        \n",
    "        self.forward()\n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            if self.init_emd is not None:\n",
    "                embedding = tf.Variable(init_emd, trainable=True, name=\"emb\", dtype=tf.float32)\n",
    "            else:\n",
    "                init_mat = tf.random_uniform([self.config.vocab_size, self.config.hidden_dim], -1, 1)\n",
    "                embedding = tf.Variable(init_mat, trainable=True, name=\"emb\", dtype=tf.float32)\n",
    "            inputs_emb = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "            # [batch_size, seq_len, embed_dim] \n",
    "            inputs_emb = tf.contrib.layers.dropout(inputs_emb, self.config.dropout_keep_prob)\n",
    "            \n",
    "        with tf.name_scope(\"convs\"):\n",
    "            convs = []\n",
    "            for kernel_size in self.config.kernel_size:\n",
    "                conv = tf.layers.conv1d(inputs_emb, \n",
    "                                        self.config.num_filters, \n",
    "                                        kernel_size, \n",
    "                                        padding='valid',\n",
    "                                        name='conv_{}'.format(kernel_size))\n",
    "                convs.append(conv)\n",
    "                \n",
    "        with tf.name_scope(\"pooling\"):\n",
    "            pool_reses = []\n",
    "            for conv_res  in convs:\n",
    "                avg_pool_res = tf.reduce_max(conv_res,  reduction_indices=[1])\n",
    "                max_pool_res = tf.reduce_mean(conv_res, reduction_indices=[1])\n",
    "                pool_reses.append(avg_pool_res)\n",
    "                pool_reses.append(max_pool_res)\n",
    "                \n",
    "            pooling_outputs = tf.concat(pool_reses, 1)\n",
    "                \n",
    "        with tf.name_scope(\"linear\"):\n",
    "            linear_res = tf.layers.dense(pooling_outputs, self.config.hidden_dim)\n",
    "            linear_res = tf.contrib.layers.dropout(linear_res, self.config.dropout_keep_prob)\n",
    "            linear_res = tf.nn.relu(linear_res)\n",
    "            \n",
    "        with tf.name_scope(\"logits\"):\n",
    "            self.logits = tf.layers.dense(linear_res, self.config.num_classes)\n",
    "            self.logits_outputs = tf.nn.softmax(self.logits)\n",
    "            self.y_pred = tf.argmax(self.logits_outputs, axis=1)\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.labels_onehot = tf.one_hot(self.input_y, depth=self.config.num_classes)\n",
    "            cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=self.labels_onehot, \n",
    "                                                            logits=self.logits) # label_smoothing=0.001\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "        with tf.name_scope(\"train_op\"):\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "        with tf.name_scope(\"acc\"):\n",
    "            _, self.cur_precision = tf.metrics.precision(self.input_y, self.y_pred)\n",
    "            \n",
    "        with tf.name_scope(\"recall\"):\n",
    "            _, self.cur_recall = tf.metrics.recall(self.input_y, self.y_pred)\n",
    "            \n",
    "        with tf.name_scope(\"f1\"):\n",
    "            self.cur_f1 = 2 * self.cur_precision * self.cur_recall / (self.cur_precision + self.cur_recall)\n",
    "        \n",
    "        with tf.name_scope(\"new_lr\"):\n",
    "            self.new_lr = tf.placeholder(tf.float32, shape=[])\n",
    "            self.lr_update = tf.assign(self.learning_rate, self.new_lr)\n",
    "        self.test = (self.train_op, self.loss)    \n",
    "    \n",
    "    def assign_lr(self, sess, lr_value):\n",
    "        sess.run(self.lr_update, feed_dict={self.new_lr: lr_value})\n",
    "                \n",
    "import config\n",
    "reload(config)\n",
    "cnn_config = config.CNNConfig\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = TextCNN(cnn_config)\n",
    "    feed_dict = {\n",
    "        model.input_x: test_x,\n",
    "        model.input_y: test_y,\n",
    "        model.keep_prob: 0.2\n",
    "        }\n",
    "    sess.run(tf.local_variables_initializer()) \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs = sess.run(model.test, feed_dict=feed_dict)\n",
    "outputs       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "three-fraud",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-03-19 14:10:05,859][INFO] ## epoch:0, fold_idx:0\n",
      "[2021-03-19 14:10:05,912][INFO] ## 0.006\n",
      "156it [00:11, 13.20it/s]\n",
      "[2021-03-19 14:10:17,735][INFO] ## train_f1:0.5386565327644348\n",
      "158it [00:03, 44.06it/s]\n",
      "[2021-03-19 14:10:21,359][INFO] ## epoch:0, eval_f1:0.7752562165260315\n",
      "[2021-03-19 14:10:21,457][INFO] ## epoch:1, fold_idx:0\n",
      "[2021-03-19 14:10:21,494][INFO] ## 0.006\n",
      "156it [00:10, 14.97it/s]\n",
      "[2021-03-19 14:10:31,916][INFO] ## train_f1:0.8027585744857788\n",
      "158it [00:03, 43.64it/s]\n",
      "[2021-03-19 14:10:35,576][INFO] ## epoch:1, eval_f1:0.7883755564689636\n",
      "[2021-03-19 14:10:35,644][INFO] ## epoch:2, fold_idx:0\n",
      "[2021-03-19 14:10:35,680][INFO] ## 0.006\n",
      "156it [00:10, 15.17it/s]\n",
      "[2021-03-19 14:10:45,967][INFO] ## train_f1:0.8788083791732788\n",
      "158it [00:03, 46.32it/s]\n",
      "[2021-03-19 14:10:49,416][INFO] ## epoch:2, eval_f1:0.8231198191642761\n",
      "[2021-03-19 14:10:49,483][INFO] ## epoch:3, fold_idx:0\n",
      "[2021-03-19 14:10:49,518][INFO] ## 0.006\n",
      "156it [00:10, 15.27it/s]\n",
      "[2021-03-19 14:10:59,735][INFO] ## train_f1:0.924313485622406\n",
      "158it [00:03, 45.65it/s]\n",
      "[2021-03-19 14:11:03,234][INFO] ## epoch:3, eval_f1:0.8025914430618286\n",
      "[2021-03-19 14:11:03,304][INFO] ## epoch:4, fold_idx:0\n",
      "[2021-03-19 14:11:03,342][INFO] ## 0.0048\n",
      "156it [00:10, 14.77it/s]\n",
      "[2021-03-19 14:11:13,908][INFO] ## train_f1:0.942875862121582\n",
      "158it [00:03, 42.59it/s]\n",
      "[2021-03-19 14:11:17,658][INFO] ## epoch:4, eval_f1:0.7281106114387512\n",
      "[2021-03-19 14:11:17,727][INFO] ## epoch:5, fold_idx:0\n",
      "[2021-03-19 14:11:17,763][INFO] ## 0.00384\n",
      "156it [00:10, 14.20it/s]\n",
      "[2021-03-19 14:11:28,752][INFO] ## train_f1:0.9384719729423523\n",
      "158it [00:03, 46.16it/s]\n",
      "[2021-03-19 14:11:32,213][INFO] ## epoch:5, eval_f1:0.8319327235221863\n",
      "[2021-03-19 14:11:32,369][INFO] ## epoch:0, fold_idx:1\n",
      "[2021-03-19 14:11:32,427][INFO] ## 0.006\n",
      "156it [00:10, 15.12it/s]\n",
      "[2021-03-19 14:11:42,744][INFO] ## train_f1:0.42139917612075806\n",
      "158it [00:03, 45.60it/s]\n",
      "[2021-03-19 14:11:46,246][INFO] ## epoch:0, eval_f1:0.6566974520683289\n",
      "[2021-03-19 14:11:46,354][INFO] ## epoch:1, fold_idx:1\n",
      "[2021-03-19 14:11:46,392][INFO] ## 0.006\n",
      "156it [00:10, 14.98it/s]\n",
      "[2021-03-19 14:11:56,804][INFO] ## train_f1:0.7208756804466248\n",
      "158it [00:03, 46.37it/s]\n",
      "[2021-03-19 14:12:00,252][INFO] ## epoch:1, eval_f1:0.7903801798820496\n",
      "[2021-03-19 14:12:00,324][INFO] ## epoch:2, fold_idx:1\n",
      "[2021-03-19 14:12:00,363][INFO] ## 0.006\n",
      "156it [00:11, 13.95it/s]\n",
      "[2021-03-19 14:12:11,550][INFO] ## train_f1:0.8618330955505371\n",
      "158it [00:03, 43.90it/s]\n",
      "[2021-03-19 14:12:15,187][INFO] ## epoch:2, eval_f1:0.8070532083511353\n",
      "[2021-03-19 14:12:15,270][INFO] ## epoch:3, fold_idx:1\n",
      "[2021-03-19 14:12:15,314][INFO] ## 0.006\n",
      "156it [00:11, 13.92it/s]\n",
      "[2021-03-19 14:12:26,528][INFO] ## train_f1:0.8994004130363464\n",
      "158it [00:03, 43.98it/s]\n",
      "[2021-03-19 14:12:30,160][INFO] ## epoch:3, eval_f1:0.8215767741203308\n",
      "[2021-03-19 14:12:30,229][INFO] ## epoch:4, fold_idx:1\n",
      "[2021-03-19 14:12:30,267][INFO] ## 0.0048\n",
      "156it [00:10, 14.64it/s]\n",
      "[2021-03-19 14:12:40,926][INFO] ## train_f1:0.9300791621208191\n",
      "158it [00:03, 47.10it/s]\n",
      "[2021-03-19 14:12:44,320][INFO] ## epoch:4, eval_f1:0.8290155529975891\n",
      "[2021-03-19 14:12:44,388][INFO] ## epoch:5, fold_idx:1\n",
      "[2021-03-19 14:12:44,423][INFO] ## 0.00384\n",
      "156it [00:10, 15.03it/s]\n",
      "[2021-03-19 14:12:54,807][INFO] ## train_f1:0.9782178997993469\n",
      "158it [00:03, 46.59it/s]\n",
      "[2021-03-19 14:12:58,239][INFO] ## epoch:5, eval_f1:0.8210217356681824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "def evaluate(sess, eval_loader, eval_allsteps):\n",
    "    total_loss = 0.0\n",
    "    total_f1 = 0.0\n",
    "    for step, (x_batch, y_batch) in tqdm(enumerate(eval_loader)):\n",
    "        if step > eval_allsteps:\n",
    "            break\n",
    "        feed_dict = {\n",
    "            model.input_x: x_batch,\n",
    "            model.input_y: y_batch,\n",
    "            model.keep_prob: 1.0\n",
    "            }\n",
    "        cur_loss, cur_f1 = sess.run([model.loss, model.cur_f1], \n",
    "                                    feed_dict=feed_dict)\n",
    "        total_loss += total_loss\n",
    "        \n",
    "    return cur_f1, cur_loss\n",
    "            \n",
    "    \n",
    "\n",
    "def train_model(config, model, all_ids, all_labels):\n",
    "    tensorboard_dir = config.tensorboard_dir\n",
    "    \n",
    "    # 配置tensorboard\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"cur_precision\", model.cur_precision)\n",
    "    tf.summary.scalar(\"cur_recall\", model.cur_recall)\n",
    "    tf.summary.scalar(\"cur_f1\", model.cur_f1)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "    \n",
    "    # 配置saver\n",
    "    saver = tf.train.Saver()\n",
    "    orig_decay = config.lr_decay\n",
    "    learning_rate = config.learning_rate\n",
    "    max_lr_epoch = config.max_lr_epoch\n",
    "    \n",
    "    # 构建训练集，验证集\n",
    "    all_labels = np.array(all_labels)\n",
    "    kf = StratifiedKFold(\n",
    "        n_splits=config.fold_count, random_state=config.seed, shuffle=True).split(X=all_ids, y=all_labels)\n",
    "    \n",
    "    # 划分训练验证集\n",
    "    for fold_idx, (train_idx, eval_idx) in enumerate(kf):\n",
    "        # 创建session\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "#         sess.run(tf.local_variables_initializer())\n",
    "        writer.add_graph(sess.graph)\n",
    "        \n",
    "        # 当前折的训练集\n",
    "        train_ids = all_ids[train_idx]\n",
    "        train_labels = all_labels[train_idx]\n",
    "        train_loader = batch_iter(train_ids, train_labels)\n",
    "        # 当前折的验证集\n",
    "        eval_ids = all_ids[eval_idx]\n",
    "        eval_labels = all_labels[eval_idx]\n",
    "        eval_loader = batch_iter(eval_ids, eval_labels, batch_size=64)\n",
    "        \n",
    "        best_f1 = -999\n",
    "        earlystop_count = 0\n",
    "        train_steps_fold = math.ceil(len(train_ids) / config.train_batch_size)\n",
    "        eval_steps_fold = math.ceil(len(eval_ids) / config.eval_batch_size)\n",
    "        for epoch in range(config.num_epochs):\n",
    "            logger.info(\"epoch:{}, fold_idx:{}\".format(epoch, fold_idx))\n",
    "            # 初始化matrics算子\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0)\n",
    "            model.assign_lr(sess, learning_rate * new_lr_decay)\n",
    "            logger.info(sess.run(model.learning_rate))\n",
    "            if earlystop_count >= config.early_stop:\n",
    "                break\n",
    "                \n",
    "            for step, (x_batch, y_batch) in tqdm(enumerate(train_loader)):\n",
    "                feed_dict = {\n",
    "                    model.input_x: x_batch,\n",
    "                    model.input_y: y_batch,\n",
    "                    model.keep_prob: config.dropout_keep_prob\n",
    "                    }\n",
    "                if step % config.save_per_batch == 0  and epoch == 0 and fold_idx == 0:\n",
    "                    # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                    s = sess.run(merged_summary, feed_dict=feed_dict)\n",
    "                    writer.add_summary(s, step)\n",
    "                    \n",
    "                # 反向传播迭代优化\n",
    "                feed_dict[model.keep_prob] = config.dropout_keep_prob\n",
    "                _, cur_f1 = sess.run([model.train_op, model.cur_f1], feed_dict=feed_dict)\n",
    "                \n",
    "                # 迭代step次时，实际已经完整的预测了traindata\n",
    "                # 如sample/bs = 62.5需要迭代63次，step=62时已经迭代63次                \n",
    "                if step + 1 >= train_steps_fold:\n",
    "                    break\n",
    "            logger.info(\"train_f1:{}\".format(cur_f1))\n",
    "            # 初始化matrics算子，进行eval\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            # 对验证集进行预测评估，并保存模型\n",
    "            eval_f1, eval_loss = evaluate(sess, eval_loader, eval_steps_fold)\n",
    "            logger.info(\"epoch:{}, eval_f1:{}\".format(epoch, eval_f1))\n",
    "            \n",
    "            if cur_f1 > best_f1:\n",
    "                best_f1 = cur_f1\n",
    "                earlystop_count = 0\n",
    "                saver.save(sess=sess, save_path=config.save_dir + config.sava_model_name + '{}'.format(fold_idx))\n",
    "            else:\n",
    "                earlystop_count += 1\n",
    "\n",
    "                \n",
    "            \n",
    "tf.reset_default_graph()\n",
    "model = TextCNN(cnn_config)    \n",
    "train_comments, train_labels = read_file(cnn_config.train_dir)\n",
    "word2id = build_vocab(train_comments)\n",
    "train_ids = convert_to_inputids(train_comments, word2id, cnn_config.seq_length)\n",
    "\n",
    "train_model(cnn_config, model, train_ids, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brilliant-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_comments, test_comment_ids = read_file(test_path, train=False)\n",
    "# test_ids = convert_to_inputids(test_comments, word2id, 50)\n",
    "# test_loader = test_batch_iter(test_ids, rnn_config.test_batch_size)\n",
    "# tf.metrics.precision.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bridal-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(config, model, test_ids):\n",
    "    \n",
    "    test_loader = test_batch_iter(test_ids, rnn_config.test_batch_size)\n",
    "    test_steps_fold = math.ceil(len(test_ids) / config.test_batch_size)\n",
    "    df_result = pd.read_csv(config.sample_dir)\n",
    "    \n",
    "    assert len(test_ids) == len(df_result), \"check your pred_data!\"\n",
    "    test_logits = np.zeros((len(df_result), 2))\n",
    "    \n",
    "    for fold_idx in range(config.fold_count):\n",
    "        model_path = config.save_dir + config.sava_model_name + '{}'.format(fold_idx)\n",
    "        print(model_path)\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess=sess, save_path=model_path)\n",
    "\n",
    "        for step, x_batch in tqdm(enumerate(test_loader)): \n",
    "                \n",
    "            start_id = step * config.test_batch_size\n",
    "            end_id = min((step + 1) * config.test_batch_size, len(df_result))\n",
    "            \n",
    "            feed_dict = {\n",
    "                model.input_x: x_batch,\n",
    "                model.input_y: 0.0,\n",
    "                model.keep_prob: 1.0\n",
    "                }\n",
    "            logits_batch, pred_batch = sess.run([model.logits_outputs, model.y_pred],\n",
    "                                                feed_dict=feed_dict)\n",
    "#             test_preds.extend(pred_batch)\n",
    "            test_logits[start_id: end_id] += logits_batch\n",
    "            \n",
    "            # 迭代step次时，实际已经完整的预测了testdata\n",
    "            # 如sample/bs = 62.5需要迭代63次，step=62时已经迭代63次\n",
    "            if step + 1 >= test_steps_fold:\n",
    "                break  \n",
    "        \n",
    "        sess.close()\n",
    "    \n",
    "    test_preds = np.argmax(test_logits, axis=1)\n",
    "    df_result[\"label\"] = test_preds\n",
    "    \n",
    "    return df_result\n",
    "    \n",
    "    \n",
    "    \n",
    "# tf.reset_default_graph()    \n",
    "# test_comments, test_comment_ids = read_file(rnn_config.test_dir, train=False)\n",
    "# test_ids = convert_to_inputids(test_comments, word2id, 50)\n",
    "# # test_loader = test_batch_iter(test_ids, rnn_config.test_batch_size)  \n",
    "# model = BiRNN(rnn_config)\n",
    "# # model_path = './best_models/rnnfold-1'\n",
    "# test_result = test_model(rnn_config, model, test_ids)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
