{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "local-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "import collections\n",
    "import re\n",
    "import jieba\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "american-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('bert-log')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\n",
    "fh = logging.FileHandler('bert.txt')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "final-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sapphire-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from importlib import reload\n",
    "import config\n",
    "reload(config)\n",
    "cnn_config = config.CNNConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fifth-touch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.647 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# def feed_data(x_batch, y_batch, keep_prob):\n",
    "#     feed_dict = {\n",
    "#         model.input_x: x_batch,\n",
    "#         model.input_y: y_batch,\n",
    "#         model.keep_prob: keep_prob\n",
    "#     }\n",
    "#     return feed_dict\n",
    "\n",
    "# def feed_data(x_batch, y_batch, keep_prob):\n",
    "#     feed_dict = {\n",
    "#         model.input_x: x_batch,\n",
    "#         model.input_y: y_batch,\n",
    "#         model.keep_prob: keep_prob\n",
    "#     }\n",
    "#     return feed_dict\n",
    "\n",
    "def read_file(file_dir, train=True):\n",
    "    \"\"\"\n",
    "    读取csv文件\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        comments, labels = [], []\n",
    "        df_train = pd.read_csv(file_dir, sep='\\t')\n",
    "        for comment, label in zip(df_train.comment, df_train.label):\n",
    "            comments.append(comment)\n",
    "            labels.append(label)\n",
    "        return comments, labels\n",
    "    else:\n",
    "        comments, comment_ids = [], []\n",
    "        df_test = pd.read_csv(file_dir)\n",
    "        for comment, comment_id in zip(df_test.comment, df_test.id):\n",
    "            comments.append(comment)\n",
    "            comment_ids.append(comment_id)\n",
    "        return comments, comment_ids\n",
    "    \n",
    "def build_vocab(train_comments, vocab_size=5000):\n",
    "    \"\"\"\n",
    "    对训练集分词,统计词频并取vocab_size个词作为词表\n",
    "    \"\"\"\n",
    "    \n",
    "    all_words = []\n",
    "    \n",
    "    for comment in train_comments:\n",
    "        seg_comment = jieba.lcut(comment)\n",
    "        all_words.extend(seg_comment)\n",
    "    word_counter = collections.Counter(all_words)\n",
    "    select_words = word_counter.most_common(vocab_size-2)\n",
    "    select_words, _ = list(zip(*select_words))\n",
    "    select_words = [\"<PAD>\"] + [\"<UNK>\"] + list(select_words)\n",
    "    word2id = {word: idx for idx, word in enumerate(select_words)}\n",
    "    \n",
    "    return word2id\n",
    "    \n",
    "def convert_to_inputids(comments, word2id, max_len=40):\n",
    "    \n",
    "    input_ids = []\n",
    "    \n",
    "    for comment in comments:\n",
    "        comment_ids = [word2id[w] if w in word2id  else word2id[\"<UNK>\"] for w in comment]\n",
    "        input_ids.append(comment_ids)\n",
    "        \n",
    "    input_ids = kr.preprocessing.sequence.pad_sequences(input_ids, max_len, padding='post')\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "def batch_iter(x, y, batch_size=32, shuffle=False):\n",
    "    \"\"\"\n",
    "    batch数据生成器\n",
    "    \"\"\"\n",
    "\n",
    "    sample_len = len(x)\n",
    "    batch_count = math.ceil(sample_len / batch_size)\n",
    "    # 随机打散\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(np.arange(sample_len))\n",
    "    else:\n",
    "        indices = list(np.arange(sample_len))\n",
    "    x_shuffle = np.array(x)[indices]\n",
    "    y_shuffle = np.array(y)[indices]\n",
    "\n",
    "    while True:\n",
    "        for i in range(batch_count):\n",
    "            start_id = i * batch_size\n",
    "            end_id = min((i + 1) * batch_size, sample_len)\n",
    "            yield x_shuffle[start_id: end_id], y_shuffle[start_id: end_id]  \n",
    "            \n",
    "def test_batch_iter(x, batch_size=32):\n",
    "    \n",
    "    sample_len = len(x)\n",
    "    batch_count = math.ceil(sample_len / batch_size)\n",
    "    \n",
    "    x = np.array(x)\n",
    "    while True:    \n",
    "        for i in range(batch_count):\n",
    "            start_id = i * batch_size\n",
    "            end_id = min((i + 1) * batch_size, sample_len)\n",
    "            yield x[start_id: end_id]\n",
    "\n",
    "train_comments, labels = read_file(cnn_config.train_dir)\n",
    "word2id = build_vocab(train_comments)\n",
    "train_ids = convert_to_inputids(train_comments, word2id, 50)\n",
    "data_iter = batch_iter(train_ids, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unique-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_comments, test_comment_ids = read_file(test_path, train=False)\n",
    "# test_ids = convert_to_inputids(test_comments, word2id, 50)\n",
    "# test_loader = test_batch_iter(test_ids, rnn_config.test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rough-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exotic-commissioner",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class BiRNN(object):\n",
    "    \n",
    "    def __init__(self, config, init_emd=None, is_training=True, is_testing=False):\n",
    "        \n",
    "        self.config = config\n",
    "        self.init_emd = init_emd\n",
    "        self.is_training = is_training\n",
    "        self.is_testing = is_testing\n",
    "#         self.learning_rate = self.config.learning_rate\n",
    "        self.learning_rate = tf.Variable(self.config.learning_rate, trainable=False)\n",
    "        \n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.int32, name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.int32, name='keep_prob')\n",
    "        \n",
    "        self.forward()\n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        def lstm_cell():\n",
    "            return tf.nn.rnn_cell.LSTMCell(self.config.hidden_dim)\n",
    "        \n",
    "        def gru_cell():\n",
    "            return tf.nn.rnn_cell.GRUCell(self.config.hidden_dim)\n",
    "        \n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            if self.init_emd is not None:\n",
    "                embedding = tf.Variable(init_emd, trainable=True, name=\"emb\", dtype=tf.float32)\n",
    "            else:\n",
    "                init_mat = tf.random_uniform([self.config.vocab_size, self.config.hidden_dim], -1, 1)\n",
    "                embedding = tf.Variable(init_mat, trainable=True, name=\"emb\", dtype=tf.float32)\n",
    "            \n",
    "            # [batch_size, seq_len, emb_dim]\n",
    "            inputs_emb = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "            \n",
    "        with tf.name_scope(\"birnn_cell\"):\n",
    "            if self.config.rnn.lower() == 'lstm':\n",
    "                cell_fw = lstm_cell()\n",
    "                cell_bw = lstm_cell()\n",
    "            else:\n",
    "                cell_fw = gru_cell()\n",
    "                cell_bw = gru_cell()\n",
    "\n",
    "            if self.is_training:\n",
    "                cell_fw = tf.nn.rnn_cell.DropoutWrapper(cell_fw, \n",
    "                                            output_keep_prob=self.config.dropout_keep_prob)\n",
    "                cell_bw = tf.nn.rnn_cell.DropoutWrapper(cell_bw, \n",
    "                                            output_keep_prob=self.config.dropout_keep_prob) \n",
    "                \n",
    "            cell_fw = tf.nn.rnn_cell.MultiRNNCell([cell_fw] * self.config.num_layers)\n",
    "            cell_bw = tf.nn.rnn_cell.MultiRNNCell([cell_bw] * self.config.num_layers)\n",
    "            \n",
    "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                          cell_bw, \n",
    "                                                                          inputs_emb, \n",
    "                                                                          dtype=tf.float32,\n",
    "                                                                          scope=\"bi-lstm\")\n",
    "            outputs = tf.concat(outputs, -1)\n",
    "            outputs = tf.reshape(outputs, \n",
    "                                 [-1, self.config.seq_length, self.config.hidden_dim * 2])\n",
    "            \n",
    "            \n",
    "        with tf.name_scope(\"pooling\"):\n",
    "            # [batch_size, 1, hid_size * 2]\n",
    "            outputs_max = tf.nn.max_pool1d(outputs,\n",
    "                                  ksize=self.config.seq_length, \n",
    "                                  strides=1, \n",
    "                                  padding=\"VALID\")\n",
    "            \n",
    "            # [batch_size, 1, hid_size * 2]\n",
    "            outputs_avg = tf.nn.avg_pool1d(outputs,\n",
    "                                  ksize=self.config.seq_length, \n",
    "                                  strides=1, \n",
    "                                  padding=\"VALID\")\n",
    "\n",
    "#             avg_pool_res = tf.reduce_max(outputs,  reduction_indices=[1])\n",
    "#             max_pool_res = tf.reduce_mean(outputs. reduction_indices=[1])\n",
    "            \n",
    "            # [batch_size, hid_size * 2]\n",
    "            outputs_max = tf.squeeze(outputs_max, 1)\n",
    "            outputs_avg = tf.squeeze(outputs_avg, 1)\n",
    "            # [batch_size, hid_size * 4]\n",
    "            pooling_outputs = tf.concat([outputs_max, outputs_avg], 1)\n",
    "            \n",
    "        with tf.name_scope(\"logits\"):\n",
    "            # [batch_size, num_classes]\n",
    "            self.logits = tf.layers.dense(pooling_outputs, self.config.num_classes)\n",
    "            self.logits_outputs = tf.nn.softmax(self.logits, axis=1)\n",
    "            self.y_pred = tf.argmax(self.logits, axis=1)\n",
    "        \n",
    "        if self.is_testing: return \n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.labels_onehot = tf.one_hot(self.input_y, depth=self.config.num_classes)\n",
    "            \n",
    "#             cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=self.labels_onehot, \n",
    "                                              logits=self.logits) # label_smoothing=0.001\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "               \n",
    "        with tf.name_scope(\"train_op\"):\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "        with tf.name_scope(\"acc\"):\n",
    "            _, self.cur_precision = tf.metrics.precision(self.input_y, self.y_pred)\n",
    "            \n",
    "        with tf.name_scope(\"recall\"):\n",
    "            _, self.cur_recall = tf.metrics.recall(self.input_y, self.y_pred)\n",
    "            \n",
    "        with tf.name_scope(\"f1\"):\n",
    "            self.cur_f1 = 2 * self.cur_precision * self.cur_recall / (self.cur_precision + self.cur_recall)\n",
    "            \n",
    "        with tf.name_scope(\"new_lr\"):\n",
    "            # 用于更新 学习率\n",
    "            self.new_lr = tf.placeholder(tf.float32, shape=[])\n",
    "            # 将new_lr赋值给lr_update\n",
    "            self.lr_update = tf.assign(self.learning_rate, self.new_lr)            \n",
    "            \n",
    "    # 更新 学习率\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self.lr_update, feed_dict={self.new_lr: lr_value})\n",
    "                \n",
    "import config\n",
    "reload(config)\n",
    "config = config.RNNConfig\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "# with tf.Session() as sess:\n",
    "#     model = BiRNN(config)\n",
    "#     feed_dict = {\n",
    "#         model.input_x: test_x,\n",
    "#         model.input_y: test_y,\n",
    "#         model.keep_prob: 0.2\n",
    "#         }\n",
    "#     sess.run(tf.local_variables_initializer()) \n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     outputs = sess.run(model.test, feed_dict=feed_dict)\n",
    "# outputs       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "three-fraud",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-03-28 19:05:50,938][INFO] ## epoch:0, fold_idx:0\n",
      "[2021-03-28 19:05:51,046][INFO] ## 0.01\n",
      "273it [00:50,  5.37it/s]\n",
      "[2021-03-28 19:06:41,848][INFO] ## train_f1:0.7142361998558044\n",
      "41it [00:03, 10.63it/s]\n",
      "[2021-03-28 19:06:45,774][INFO] ## epoch:0, eval_f1:0.854521632194519\n",
      "[2021-03-28 19:06:45,915][INFO] ## epoch:1, fold_idx:0\n",
      "[2021-03-28 19:06:45,979][INFO] ## 0.01\n",
      "273it [00:45,  5.99it/s]\n",
      "[2021-03-28 19:07:31,571][INFO] ## train_f1:0.8598276972770691\n",
      "41it [00:02, 13.95it/s]\n",
      "[2021-03-28 19:07:34,587][INFO] ## epoch:1, eval_f1:0.8362184166908264\n",
      "[2021-03-28 19:07:34,695][INFO] ## epoch:2, fold_idx:0\n",
      "[2021-03-28 19:07:34,766][INFO] ## 0.01\n",
      "273it [00:47,  5.75it/s]\n",
      "[2021-03-28 19:08:22,223][INFO] ## train_f1:0.8931739330291748\n",
      "41it [00:02, 14.03it/s]\n",
      "[2021-03-28 19:08:25,223][INFO] ## epoch:2, eval_f1:0.8475034236907959\n",
      "[2021-03-28 19:08:25,343][INFO] ## epoch:3, fold_idx:0\n",
      "[2021-03-28 19:08:25,410][INFO] ## 0.01\n",
      "273it [00:44,  6.17it/s]\n",
      "[2021-03-28 19:09:09,642][INFO] ## train_f1:0.9205500483512878\n",
      "41it [00:02, 15.06it/s]\n",
      "[2021-03-28 19:09:12,439][INFO] ## epoch:3, eval_f1:0.8335745334625244\n",
      "[2021-03-28 19:09:12,547][INFO] ## epoch:4, fold_idx:0\n",
      "[2021-03-28 19:09:12,617][INFO] ## 0.008\n",
      "273it [00:45,  6.05it/s]\n",
      "[2021-03-28 19:09:57,752][INFO] ## train_f1:0.9470475912094116\n",
      "41it [00:02, 15.07it/s]\n",
      "[2021-03-28 19:10:00,542][INFO] ## epoch:4, eval_f1:0.8148148059844971\n",
      "[2021-03-28 19:10:00,653][INFO] ## epoch:5, fold_idx:0\n",
      "[2021-03-28 19:10:00,722][INFO] ## 0.0064\n",
      "273it [00:45,  6.03it/s]\n",
      "[2021-03-28 19:10:46,011][INFO] ## train_f1:0.9733638167381287\n",
      "41it [00:02, 15.30it/s]\n",
      "[2021-03-28 19:10:48,760][INFO] ## epoch:5, eval_f1:0.8619791865348816\n",
      "[2021-03-28 19:10:49,048][INFO] ## epoch:0, fold_idx:1\n",
      "[2021-03-28 19:10:49,154][INFO] ## 0.01\n",
      "273it [00:49,  5.56it/s]\n",
      "[2021-03-28 19:11:38,257][INFO] ## train_f1:0.6695427298545837\n",
      "41it [00:03, 13.51it/s]\n",
      "[2021-03-28 19:11:41,366][INFO] ## epoch:0, eval_f1:0.8053097128868103\n",
      "[2021-03-28 19:11:41,510][INFO] ## epoch:1, fold_idx:1\n",
      "[2021-03-28 19:11:41,579][INFO] ## 0.01\n",
      "273it [00:45,  6.06it/s]\n",
      "[2021-03-28 19:12:26,644][INFO] ## train_f1:0.8535031676292419\n",
      "41it [00:02, 14.48it/s]\n",
      "[2021-03-28 19:12:29,550][INFO] ## epoch:1, eval_f1:0.8207283020019531\n",
      "[2021-03-28 19:12:29,654][INFO] ## epoch:2, fold_idx:1\n",
      "[2021-03-28 19:12:29,719][INFO] ## 0.01\n",
      "273it [00:45,  5.94it/s]\n",
      "[2021-03-28 19:13:15,700][INFO] ## train_f1:0.8955107927322388\n",
      "41it [00:02, 14.99it/s]\n",
      "[2021-03-28 19:13:18,507][INFO] ## epoch:2, eval_f1:0.7816993594169617\n",
      "[2021-03-28 19:13:18,609][INFO] ## epoch:3, fold_idx:1\n",
      "[2021-03-28 19:13:18,675][INFO] ## 0.01\n",
      "273it [00:45,  6.04it/s]\n",
      "[2021-03-28 19:14:03,856][INFO] ## train_f1:0.9234320521354675\n",
      "41it [00:02, 14.99it/s]\n",
      "[2021-03-28 19:14:06,664][INFO] ## epoch:3, eval_f1:0.8139535188674927\n",
      "[2021-03-28 19:14:06,769][INFO] ## epoch:4, fold_idx:1\n",
      "[2021-03-28 19:14:06,842][INFO] ## 0.008\n",
      "273it [00:44,  6.09it/s]\n",
      "[2021-03-28 19:14:51,689][INFO] ## train_f1:0.9447197914123535\n",
      "41it [00:02, 15.40it/s]\n",
      "[2021-03-28 19:14:54,423][INFO] ## epoch:4, eval_f1:0.8218830227851868\n",
      "[2021-03-28 19:14:54,529][INFO] ## epoch:5, fold_idx:1\n",
      "[2021-03-28 19:14:54,595][INFO] ## 0.0064\n",
      "273it [00:44,  6.13it/s]\n",
      "[2021-03-28 19:15:39,149][INFO] ## train_f1:0.9620637893676758\n",
      "41it [00:02, 14.21it/s]\n",
      "[2021-03-28 19:15:42,107][INFO] ## epoch:5, eval_f1:0.8127599954605103\n",
      "[2021-03-28 19:15:42,396][INFO] ## epoch:0, fold_idx:2\n",
      "[2021-03-28 19:15:42,505][INFO] ## 0.01\n",
      "273it [00:48,  5.67it/s]\n",
      "[2021-03-28 19:16:30,674][INFO] ## train_f1:0.7015050053596497\n",
      "41it [00:03, 13.35it/s]\n",
      "[2021-03-28 19:16:33,816][INFO] ## epoch:0, eval_f1:0.8440860509872437\n",
      "[2021-03-28 19:16:33,965][INFO] ## epoch:1, fold_idx:2\n",
      "[2021-03-28 19:16:34,032][INFO] ## 0.01\n",
      "273it [00:44,  6.12it/s]\n",
      "[2021-03-28 19:17:18,638][INFO] ## train_f1:0.8568074703216553\n",
      "41it [00:02, 15.11it/s]\n",
      "[2021-03-28 19:17:21,423][INFO] ## epoch:1, eval_f1:0.8343023657798767\n",
      "[2021-03-28 19:17:21,528][INFO] ## epoch:2, fold_idx:2\n",
      "[2021-03-28 19:17:21,593][INFO] ## 0.01\n",
      "273it [00:46,  5.91it/s]\n",
      "[2021-03-28 19:18:07,753][INFO] ## train_f1:0.8959448337554932\n",
      "41it [00:02, 15.09it/s]\n",
      "[2021-03-28 19:18:10,542][INFO] ## epoch:2, eval_f1:0.8583450317382812\n",
      "[2021-03-28 19:18:10,653][INFO] ## epoch:3, fold_idx:2\n",
      "[2021-03-28 19:18:10,723][INFO] ## 0.01\n",
      "273it [00:45,  6.02it/s]\n",
      "[2021-03-28 19:18:56,086][INFO] ## train_f1:0.9278587102890015\n",
      "41it [00:02, 15.06it/s]\n",
      "[2021-03-28 19:18:58,881][INFO] ## epoch:3, eval_f1:0.8617449402809143\n",
      "[2021-03-28 19:18:58,987][INFO] ## epoch:4, fold_idx:2\n",
      "[2021-03-28 19:18:59,053][INFO] ## 0.008\n",
      "273it [00:46,  5.91it/s]\n",
      "[2021-03-28 19:19:45,250][INFO] ## train_f1:0.9530354738235474\n",
      "41it [00:02, 14.22it/s]\n",
      "[2021-03-28 19:19:48,208][INFO] ## epoch:4, eval_f1:0.8690807819366455\n",
      "[2021-03-28 19:19:48,320][INFO] ## epoch:5, fold_idx:2\n",
      "[2021-03-28 19:19:48,390][INFO] ## 0.0064\n",
      "273it [00:44,  6.18it/s]\n",
      "[2021-03-28 19:20:32,552][INFO] ## train_f1:0.9772036075592041\n",
      "41it [00:02, 14.95it/s]\n",
      "[2021-03-28 19:20:35,374][INFO] ## epoch:5, eval_f1:0.840425431728363\n",
      "[2021-03-28 19:20:35,676][INFO] ## epoch:0, fold_idx:3\n",
      "[2021-03-28 19:20:35,789][INFO] ## 0.01\n",
      "273it [00:46,  5.85it/s]\n",
      "[2021-03-28 19:21:22,486][INFO] ## train_f1:0.7215620279312134\n",
      "41it [00:02, 14.21it/s]\n",
      "[2021-03-28 19:21:25,447][INFO] ## epoch:0, eval_f1:0.7859237790107727\n",
      "[2021-03-28 19:21:25,593][INFO] ## epoch:1, fold_idx:3\n",
      "[2021-03-28 19:21:25,660][INFO] ## 0.01\n",
      "273it [00:47,  5.71it/s]\n",
      "[2021-03-28 19:22:13,454][INFO] ## train_f1:0.8580417633056641\n",
      "41it [00:02, 14.83it/s]\n",
      "[2021-03-28 19:22:16,290][INFO] ## epoch:1, eval_f1:0.8268657326698303\n",
      "[2021-03-28 19:22:16,404][INFO] ## epoch:2, fold_idx:3\n",
      "[2021-03-28 19:22:16,478][INFO] ## 0.01\n",
      "273it [00:46,  5.88it/s]\n",
      "[2021-03-28 19:23:02,883][INFO] ## train_f1:0.9053846597671509\n",
      "41it [00:02, 13.95it/s]\n",
      "[2021-03-28 19:23:05,895][INFO] ## epoch:2, eval_f1:0.8262109160423279\n",
      "[2021-03-28 19:23:06,009][INFO] ## epoch:3, fold_idx:3\n",
      "[2021-03-28 19:23:06,080][INFO] ## 0.01\n",
      "273it [00:46,  5.92it/s]\n",
      "[2021-03-28 19:23:52,196][INFO] ## train_f1:0.9192307591438293\n",
      "41it [00:02, 14.09it/s]\n",
      "[2021-03-28 19:23:55,181][INFO] ## epoch:3, eval_f1:0.8220690488815308\n",
      "[2021-03-28 19:23:55,296][INFO] ## epoch:4, fold_idx:3\n",
      "[2021-03-28 19:23:55,367][INFO] ## 0.008\n",
      "273it [00:44,  6.11it/s]\n",
      "[2021-03-28 19:24:40,083][INFO] ## train_f1:0.9360730648040771\n",
      "41it [00:02, 14.92it/s]\n",
      "[2021-03-28 19:24:42,906][INFO] ## epoch:4, eval_f1:0.8194842338562012\n",
      "[2021-03-28 19:24:43,012][INFO] ## epoch:5, fold_idx:3\n",
      "[2021-03-28 19:24:43,080][INFO] ## 0.0064\n",
      "273it [00:45,  5.96it/s]\n",
      "[2021-03-28 19:25:28,877][INFO] ## train_f1:0.9624573588371277\n",
      "41it [00:02, 15.15it/s]\n",
      "[2021-03-28 19:25:31,654][INFO] ## epoch:5, eval_f1:0.8262654542922974\n",
      "[2021-03-28 19:25:31,955][INFO] ## epoch:0, fold_idx:4\n",
      "[2021-03-28 19:25:32,072][INFO] ## 0.01\n",
      "273it [00:47,  5.81it/s]\n",
      "[2021-03-28 19:26:19,090][INFO] ## train_f1:0.7061811685562134\n",
      "41it [00:02, 14.42it/s]\n",
      "[2021-03-28 19:26:22,006][INFO] ## epoch:0, eval_f1:0.7936962842941284\n",
      "[2021-03-28 19:26:22,146][INFO] ## epoch:1, fold_idx:4\n",
      "[2021-03-28 19:26:22,213][INFO] ## 0.01\n",
      "273it [00:45,  5.96it/s]\n",
      "[2021-03-28 19:27:08,023][INFO] ## train_f1:0.8572567701339722\n",
      "41it [00:02, 14.88it/s]\n",
      "[2021-03-28 19:27:10,853][INFO] ## epoch:1, eval_f1:0.835579514503479\n",
      "[2021-03-28 19:27:10,959][INFO] ## epoch:2, fold_idx:4\n",
      "[2021-03-28 19:27:11,027][INFO] ## 0.01\n",
      "273it [00:46,  5.90it/s]\n",
      "[2021-03-28 19:27:57,293][INFO] ## train_f1:0.8994585871696472\n",
      "41it [00:02, 14.19it/s]\n",
      "[2021-03-28 19:28:00,258][INFO] ## epoch:2, eval_f1:0.8219895362854004\n",
      "[2021-03-28 19:28:00,368][INFO] ## epoch:3, fold_idx:4\n",
      "[2021-03-28 19:28:00,436][INFO] ## 0.01\n",
      "273it [00:45,  6.05it/s]\n",
      "[2021-03-28 19:28:45,548][INFO] ## train_f1:0.9326186776161194\n",
      "41it [00:02, 15.13it/s]\n",
      "[2021-03-28 19:28:48,329][INFO] ## epoch:3, eval_f1:0.807881772518158\n",
      "[2021-03-28 19:28:48,439][INFO] ## epoch:4, fold_idx:4\n",
      "[2021-03-28 19:28:48,507][INFO] ## 0.008\n",
      "273it [00:45,  5.96it/s]\n",
      "[2021-03-28 19:29:34,282][INFO] ## train_f1:0.9416698217391968\n",
      "41it [00:02, 15.04it/s]\n",
      "[2021-03-28 19:29:37,082][INFO] ## epoch:4, eval_f1:0.8531073331832886\n",
      "[2021-03-28 19:29:37,191][INFO] ## epoch:5, fold_idx:4\n",
      "[2021-03-28 19:29:37,265][INFO] ## 0.0064\n",
      "273it [00:45,  5.97it/s]\n",
      "[2021-03-28 19:30:22,959][INFO] ## train_f1:0.9681093096733093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:02, 15.06it/s]\n",
      "[2021-03-28 19:30:25,760][INFO] ## epoch:5, eval_f1:0.8085106015205383\n",
      "[2021-03-28 19:30:26,066][INFO] ## epoch:0, fold_idx:5\n",
      "[2021-03-28 19:30:26,181][INFO] ## 0.01\n",
      "273it [00:46,  5.87it/s]\n",
      "[2021-03-28 19:31:12,660][INFO] ## train_f1:0.6934801340103149\n",
      "41it [00:03, 12.98it/s]\n",
      "[2021-03-28 19:31:15,894][INFO] ## epoch:0, eval_f1:0.8108108043670654\n",
      "[2021-03-28 19:31:16,038][INFO] ## epoch:1, fold_idx:5\n",
      "[2021-03-28 19:31:16,105][INFO] ## 0.01\n",
      "273it [00:45,  6.02it/s]\n",
      "[2021-03-28 19:32:01,490][INFO] ## train_f1:0.8486528992652893\n",
      "41it [00:02, 14.10it/s]\n",
      "[2021-03-28 19:32:04,473][INFO] ## epoch:1, eval_f1:0.8355554938316345\n",
      "[2021-03-28 19:32:04,580][INFO] ## epoch:2, fold_idx:5\n",
      "[2021-03-28 19:32:04,650][INFO] ## 0.01\n",
      "273it [00:47,  5.71it/s]\n",
      "[2021-03-28 19:32:52,481][INFO] ## train_f1:0.8958009481430054\n",
      "41it [00:02, 14.88it/s]\n",
      "[2021-03-28 19:32:55,314][INFO] ## epoch:2, eval_f1:0.8504801392555237\n",
      "[2021-03-28 19:32:55,430][INFO] ## epoch:3, fold_idx:5\n",
      "[2021-03-28 19:32:55,501][INFO] ## 0.01\n",
      "273it [00:46,  5.83it/s]\n",
      "[2021-03-28 19:33:42,310][INFO] ## train_f1:0.9140444397926331\n",
      "41it [00:02, 14.91it/s]\n",
      "[2021-03-28 19:33:45,137][INFO] ## epoch:3, eval_f1:0.8547008037567139\n",
      "[2021-03-28 19:33:45,253][INFO] ## epoch:4, fold_idx:5\n",
      "[2021-03-28 19:33:45,336][INFO] ## 0.008\n",
      "273it [00:46,  5.91it/s]\n",
      "[2021-03-28 19:34:31,523][INFO] ## train_f1:0.944845974445343\n",
      "41it [00:02, 14.96it/s]\n",
      "[2021-03-28 19:34:34,340][INFO] ## epoch:4, eval_f1:0.8424068093299866\n",
      "[2021-03-28 19:34:34,452][INFO] ## epoch:5, fold_idx:5\n",
      "[2021-03-28 19:34:34,523][INFO] ## 0.0064\n",
      "273it [00:46,  5.90it/s]\n",
      "[2021-03-28 19:35:20,775][INFO] ## train_f1:0.9641768336296082\n",
      "41it [00:02, 14.13it/s]\n",
      "[2021-03-28 19:35:23,749][INFO] ## epoch:5, eval_f1:0.8469945192337036\n",
      "[2021-03-28 19:35:24,065][INFO] ## epoch:0, fold_idx:6\n",
      "[2021-03-28 19:35:24,183][INFO] ## 0.01\n",
      "273it [00:45,  5.94it/s]\n",
      "[2021-03-28 19:36:10,120][INFO] ## train_f1:0.695059597492218\n",
      "41it [00:03, 13.48it/s]\n",
      "[2021-03-28 19:36:13,238][INFO] ## epoch:0, eval_f1:0.808080792427063\n",
      "[2021-03-28 19:36:13,388][INFO] ## epoch:1, fold_idx:6\n",
      "[2021-03-28 19:36:13,462][INFO] ## 0.01\n",
      "273it [00:45,  6.02it/s]\n",
      "[2021-03-28 19:36:58,803][INFO] ## train_f1:0.8606395721435547\n",
      "41it [00:02, 15.08it/s]\n",
      "[2021-03-28 19:37:01,595][INFO] ## epoch:1, eval_f1:0.8180495500564575\n",
      "[2021-03-28 19:37:01,696][INFO] ## epoch:2, fold_idx:6\n",
      "[2021-03-28 19:37:01,770][INFO] ## 0.01\n",
      "273it [00:46,  5.85it/s]\n",
      "[2021-03-28 19:37:48,476][INFO] ## train_f1:0.9013107419013977\n",
      "41it [00:02, 14.07it/s]\n",
      "[2021-03-28 19:37:51,470][INFO] ## epoch:2, eval_f1:0.8039215207099915\n",
      "[2021-03-28 19:37:51,583][INFO] ## epoch:3, fold_idx:6\n",
      "[2021-03-28 19:37:51,653][INFO] ## 0.01\n",
      "273it [00:45,  5.95it/s]\n",
      "[2021-03-28 19:38:37,544][INFO] ## train_f1:0.9307839870452881\n",
      "41it [00:02, 15.03it/s]\n",
      "[2021-03-28 19:38:40,347][INFO] ## epoch:3, eval_f1:0.8074368834495544\n",
      "[2021-03-28 19:38:40,452][INFO] ## epoch:4, fold_idx:6\n",
      "[2021-03-28 19:38:40,523][INFO] ## 0.008\n",
      "273it [00:46,  5.82it/s]\n",
      "[2021-03-28 19:39:27,398][INFO] ## train_f1:0.9296635985374451\n",
      "41it [00:02, 14.20it/s]\n",
      "[2021-03-28 19:39:30,365][INFO] ## epoch:4, eval_f1:0.8079469799995422\n",
      "[2021-03-28 19:39:30,366][INFO] ## epoch:5, fold_idx:6\n",
      "[2021-03-28 19:39:30,441][INFO] ## 0.0064\n",
      "273it [00:44,  6.15it/s]\n",
      "[2021-03-28 19:40:14,797][INFO] ## train_f1:0.9639468193054199\n",
      "41it [00:02, 14.10it/s]\n",
      "[2021-03-28 19:40:17,781][INFO] ## epoch:5, eval_f1:0.8177905678749084\n",
      "[2021-03-28 19:40:18,087][INFO] ## epoch:0, fold_idx:7\n",
      "[2021-03-28 19:40:18,205][INFO] ## 0.01\n",
      "273it [00:46,  5.85it/s]\n",
      "[2021-03-28 19:41:04,879][INFO] ## train_f1:0.7177554368972778\n",
      "41it [00:02, 14.10it/s]\n",
      "[2021-03-28 19:41:07,863][INFO] ## epoch:0, eval_f1:0.8297567963600159\n",
      "[2021-03-28 19:41:08,010][INFO] ## epoch:1, fold_idx:7\n",
      "[2021-03-28 19:41:08,084][INFO] ## 0.01\n",
      "273it [00:46,  5.86it/s]\n",
      "[2021-03-28 19:41:54,644][INFO] ## train_f1:0.8598350882530212\n",
      "41it [00:02, 14.88it/s]\n",
      "[2021-03-28 19:41:57,475][INFO] ## epoch:1, eval_f1:0.8495821356773376\n",
      "[2021-03-28 19:41:57,575][INFO] ## epoch:2, fold_idx:7\n",
      "[2021-03-28 19:41:57,646][INFO] ## 0.01\n",
      "273it [00:46,  5.83it/s]\n",
      "[2021-03-28 19:42:44,497][INFO] ## train_f1:0.9107005596160889\n",
      "41it [00:02, 14.26it/s]\n",
      "[2021-03-28 19:42:47,452][INFO] ## epoch:2, eval_f1:0.8535912036895752\n",
      "[2021-03-28 19:42:47,563][INFO] ## epoch:3, fold_idx:7\n",
      "[2021-03-28 19:42:47,641][INFO] ## 0.01\n",
      "273it [00:46,  5.92it/s]\n",
      "[2021-03-28 19:43:33,766][INFO] ## train_f1:0.9240214824676514\n",
      "41it [00:02, 14.30it/s]\n",
      "[2021-03-28 19:43:36,714][INFO] ## epoch:3, eval_f1:0.8567530512809753\n",
      "[2021-03-28 19:43:36,824][INFO] ## epoch:4, fold_idx:7\n",
      "[2021-03-28 19:43:36,896][INFO] ## 0.008\n",
      "273it [00:44,  6.13it/s]\n",
      "[2021-03-28 19:44:21,456][INFO] ## train_f1:0.9360888004302979\n",
      "41it [00:02, 14.12it/s]\n",
      "[2021-03-28 19:44:24,435][INFO] ## epoch:4, eval_f1:0.8467966914176941\n",
      "[2021-03-28 19:44:24,546][INFO] ## epoch:5, fold_idx:7\n",
      "[2021-03-28 19:44:24,615][INFO] ## 0.0064\n",
      "273it [00:45,  6.02it/s]\n",
      "[2021-03-28 19:45:09,986][INFO] ## train_f1:0.9623430371284485\n",
      "41it [00:02, 14.98it/s]\n",
      "[2021-03-28 19:45:12,800][INFO] ## epoch:5, eval_f1:0.8625850081443787\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "def evaluate(sess, eval_loader, eval_allsteps):\n",
    "    total_loss = 0.0\n",
    "    total_f1 = 0.0\n",
    "    for step, (x_batch, y_batch) in tqdm(enumerate(eval_loader)):\n",
    "        if step > eval_allsteps:\n",
    "            break\n",
    "        feed_dict = {\n",
    "            model.input_x: x_batch,\n",
    "            model.input_y: y_batch,\n",
    "            model.keep_prob: 1.0\n",
    "            }\n",
    "        cur_loss, cur_f1 = sess.run([model.loss, model.cur_f1], \n",
    "                                    feed_dict=feed_dict)\n",
    "        total_loss += total_loss\n",
    "        \n",
    "    return cur_f1, cur_loss\n",
    "            \n",
    "    \n",
    "\n",
    "def train_model(config, model, all_ids, all_labels):\n",
    "    tensorboard_dir = config.tensorboard_dir\n",
    "    \n",
    "    # 配置tensorboard\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"cur_precision\", model.cur_precision)\n",
    "    tf.summary.scalar(\"cur_recall\", model.cur_recall)\n",
    "    tf.summary.scalar(\"cur_f1\", model.cur_f1)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "    \n",
    "    # 配置saver\n",
    "    saver = tf.train.Saver()\n",
    "    orig_decay = config.lr_decay\n",
    "    learning_rate = config.learning_rate\n",
    "    max_lr_epoch = config.max_lr_epoch\n",
    "    \n",
    "    # 构建训练集，验证集\n",
    "    all_labels = np.array(all_labels)\n",
    "    kf = StratifiedKFold(\n",
    "        n_splits=config.fold_count, random_state=config.seed, shuffle=True).split(X=all_ids, y=all_labels)\n",
    "    \n",
    "    # 划分训练验证集\n",
    "    for fold_idx, (train_idx, eval_idx) in enumerate(kf):\n",
    "        # 创建session\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "#         sess.run(tf.local_variables_initializer())\n",
    "        writer.add_graph(sess.graph)\n",
    "        \n",
    "        # 当前折的训练集\n",
    "        train_ids = all_ids[train_idx]\n",
    "        train_labels = all_labels[train_idx]\n",
    "        train_loader = batch_iter(train_ids, train_labels)\n",
    "        # 当前折的验证集\n",
    "        eval_ids = all_ids[eval_idx]\n",
    "        eval_labels = all_labels[eval_idx]\n",
    "        eval_loader = batch_iter(eval_ids, eval_labels, batch_size=64)\n",
    "        \n",
    "        best_f1 = -999\n",
    "        earlystop_count = 0\n",
    "        train_steps_fold = math.ceil(len(train_ids) / config.train_batch_size)\n",
    "        eval_steps_fold = math.ceil(len(eval_ids) / config.eval_batch_size)\n",
    "        for epoch in range(config.num_epochs):\n",
    "            logger.info(\"epoch:{}, fold_idx:{}\".format(epoch, fold_idx))\n",
    "            # 初始化matrics算子\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0)\n",
    "            model.assign_lr(sess, learning_rate * new_lr_decay)\n",
    "            logger.info(sess.run(model.learning_rate))\n",
    "            if earlystop_count >= config.early_stop:\n",
    "                break\n",
    "                \n",
    "            for step, (x_batch, y_batch) in tqdm(enumerate(train_loader)):\n",
    "                feed_dict = {\n",
    "                    model.input_x: x_batch,\n",
    "                    model.input_y: y_batch,\n",
    "                    model.keep_prob: config.dropout_keep_prob\n",
    "                    }\n",
    "                if step % config.save_per_batch == 0  and epoch == 0 and fold_idx == 0:\n",
    "                    # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                    s = sess.run(merged_summary, feed_dict=feed_dict)\n",
    "                    writer.add_summary(s, step)\n",
    "                    \n",
    "                # 反向传播迭代优化\n",
    "                feed_dict[model.keep_prob] = config.dropout_keep_prob\n",
    "                _, cur_f1 = sess.run([model.train_op, model.cur_f1], feed_dict=feed_dict)\n",
    "                \n",
    "                # 迭代step次时，实际已经完整的预测了traindata\n",
    "                # 如sample/bs = 62.5需要迭代63次，step=62时已经迭代63次                \n",
    "                if step + 1 >= train_steps_fold:\n",
    "                    break\n",
    "            logger.info(\"train_f1:{}\".format(cur_f1))\n",
    "            # 初始化matrics算子，进行eval\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            # 对验证集进行预测评估，并保存模型\n",
    "            eval_f1, eval_loss = evaluate(sess, eval_loader, eval_steps_fold)\n",
    "            logger.info(\"epoch:{}, eval_f1:{}\".format(epoch, eval_f1))\n",
    "            \n",
    "            if cur_f1 > best_f1:\n",
    "                best_f1 = cur_f1\n",
    "                earlystop_count = 0\n",
    "                saver.save(sess=sess, save_path=config.save_dir + config.sava_model_name + '{}'.format(fold_idx))\n",
    "            else:\n",
    "                earlystop_count += 1\n",
    "\n",
    "                \n",
    "            \n",
    "tf.reset_default_graph()\n",
    "model = BiRNN(config)    \n",
    "train_comments, train_labels = read_file(config.train_dir)\n",
    "word2id = build_vocab(train_comments)\n",
    "train_ids = convert_to_inputids(train_comments, word2id, cnn_config.seq_length)\n",
    "\n",
    "train_model(config, model, train_ids, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_comments, test_comment_ids = read_file(test_path, train=False)\n",
    "# test_ids = convert_to_inputids(test_comments, word2id, 50)\n",
    "# test_loader = test_batch_iter(test_ids, rnn_config.test_batch_size)\n",
    "# tf.metrics.precision.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bridal-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(config, model, test_ids):\n",
    "    \n",
    "    test_loader = test_batch_iter(test_ids, config.test_batch_size)\n",
    "    test_steps_fold = math.ceil(len(test_ids) / config.test_batch_size)\n",
    "    df_result = pd.read_csv(config.sample_dir)\n",
    "    \n",
    "    assert len(test_ids) == len(df_result), \"check your pred_data!\"\n",
    "    test_logits = np.zeros((len(df_result), 2))\n",
    "    \n",
    "    for fold_idx in range(config.fold_count):\n",
    "        model_path = config.save_dir + config.sava_model_name + '{}'.format(fold_idx)\n",
    "        print(model_path)\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess=sess, save_path=model_path)\n",
    "\n",
    "        for step, x_batch in tqdm(enumerate(test_loader)): \n",
    "                \n",
    "            start_id = step * config.test_batch_size\n",
    "            end_id = min((step + 1) * config.test_batch_size, len(df_result))\n",
    "            \n",
    "            feed_dict = {\n",
    "                model.input_x: x_batch,\n",
    "                model.input_y: 0.0,\n",
    "                model.keep_prob: 1.0\n",
    "                }\n",
    "            logits_batch, pred_batch = sess.run([model.logits_outputs, model.y_pred],\n",
    "                                                feed_dict=feed_dict)\n",
    "#             test_preds.extend(pred_batch)\n",
    "            test_logits[start_id: end_id] += logits_batch\n",
    "            \n",
    "            # 迭代step次时，实际已经完整的预测了testdata\n",
    "            # 如sample/bs = 62.5需要迭代63次，step=62时已经迭代63次\n",
    "            if step + 1 >= test_steps_fold:\n",
    "                break  \n",
    "        \n",
    "        sess.close()\n",
    "    \n",
    "    test_preds = np.argmax(test_logits, axis=1)\n",
    "    df_result[\"label\"] = test_preds\n",
    "    \n",
    "    return df_result\n",
    "    \n",
    "    \n",
    "    \n",
    "# tf.reset_default_graph()    \n",
    "# test_comments, test_comment_ids = read_file(config.test_dir, train=False)\n",
    "# test_ids = convert_to_inputids(test_comments, word2id, 50)\n",
    "# # test_loader = test_batch_iter(test_ids, rnn_config.test_batch_size)  \n",
    "# model = BiRNN(config)\n",
    "# # model_path = './best_models/rnnfold-1'\n",
    "# test_result = test_model(config, model, test_ids)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-badge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
